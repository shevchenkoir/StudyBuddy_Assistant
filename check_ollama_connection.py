# # # import requests
# # # import time
# # # from pprint import pprint

# # # OLLAMA_URL = "http://localhost:11434"
# # # MODEL_NAME = "llama3.1:latest"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä—Å–∏—é
# # # TIMEOUT = 15  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º

# # # def check_server():
# # #     """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞ Ollama"""
# # #     try:
# # #         start_time = time.time()
# # #         response = requests.get(f"{OLLAMA_URL}/", timeout=TIMEOUT)
# # #         elapsed = (time.time() - start_time) * 1000
        
# # #         if response.status_code == 200:
# # #             print(f"‚úÖ –°–µ—Ä–≤–µ—Ä Ollama –¥–æ—Å—Ç—É–ø–µ–Ω (–ø–æ—Ä—Ç 11434). –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {elapsed:.0f}–º—Å")
# # #             print(f"–í–µ—Ä—Å–∏—è —Å–µ—Ä–≤–µ—Ä–∞: {response.text.strip()}")
# # #             return True
        
# # #         print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: HTTP {response.status_code}")
# # #         return False

# # #     except requests.exceptions.RequestException as e:
# # #         print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {type(e).__name__}")
# # #         print("‚Üí –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ:")
# # #         print("1. –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω: ollama serve")
# # #         print("2. –ü–æ—Ä—Ç 11434 –Ω–µ –∑–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º")
# # #         return False

# # # def check_model():
# # #     """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
# # #     try:
# # #         print(f"\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å '{MODEL_NAME}'...")
        
# # #         # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
# # #         models = requests.get(f"{OLLAMA_URL}/api/tags", timeout=TIMEOUT).json()
# # #         if not any(m['name'] == MODEL_NAME for m in models.get('models', [])):
# # #             print(f"‚ùå –ú–æ–¥–µ–ª—å '{MODEL_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ")
# # #             print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
# # #             pprint(models)
# # #             return False

# # #         # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
# # #         start_time = time.time()
# # #         response = requests.post(
# # #             f"{OLLAMA_URL}/api/generate",
# # #             json={
# # #                 "model": MODEL_NAME,
# # #                 "prompt": "–ù–∞–ø–∏—à–∏ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ Python",
# # #                 "stream": False
# # #             },
# # #             timeout=TIMEOUT
# # #         )
# # #         elapsed = (time.time() - start_time) * 1000

# # #         if response.status_code == 200:
# # #             result = response.json()
# # #             print(f"‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç. –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {elapsed:.0f}–º—Å")
# # #             print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {result['response']}")
# # #             return True
            
# # #         print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: HTTP {response.status_code}")
# # #         pprint(response.json())
# # #         return False

# # #     except requests.exceptions.RequestException as e:
# # #         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏: {type(e).__name__}")
# # #         print(f"‚Üí –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: ollama run {MODEL_NAME}")
# # #         return False

# # # if __name__ == "__main__":
# # #     print(f"üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama ({OLLAMA_URL})...")
# # #     if check_server():
# # #         check_model()


# # import requests
# # import time
# # from pprint import pprint

# # OLLAMA_URL = "http://localhost:11434"
# # MODEL_NAME = "llama3.1:latest"
# # TIMEOUT = 60  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º

# # def check_connection():
# #     print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama ({OLLAMA_URL})")
    
# #     try:
# #         # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
# #         start = time.time()
# #         resp = requests.get(f"{OLLAMA_URL}/", timeout=10)
# #         print(f"‚úÖ –°–µ—Ä–≤–µ—Ä: HTTP {resp.status_code} ({time.time()-start:.1f}s)")
# #         print(f"–í–µ—Ä—Å–∏—è: {resp.text.strip()}")

# #         # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
# #         models = requests.get(f"{OLLAMA_URL}/api/tags", timeout=10).json()
# #         print(f"\nüìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
# #         for model in models.get('models', []):
# #             print(f"- {model['name']} ({model['size']/1e9:.1f}GB)")

# #         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# #         print(f"\nüî• –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞...")
# #         start = time.time()
# #         response = requests.post(
# #             f"{OLLAMA_URL}/api/generate",
# #             json={
# #                 "model": MODEL_NAME,
# #                 "prompt": "–û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: –ö–∞–∫ –¥–µ–ª–∞?",
# #                 "stream": False
# #             },
# #             timeout=TIMEOUT
# #         )
        
# #         if response.status_code == 200:
# #             print(f"‚úÖ –£—Å–ø–µ—Ö! –û—Ç–≤–µ—Ç –∑–∞ {time.time()-start:.1f}s:")
# #             print(response.json().get('response', '–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç'))
# #         else:
# #             print(f"‚ùå –û—à–∏–±–∫–∞ HTTP {response.status_code}:")
# #             pprint(response.json())

# #     except requests.exceptions.Timeout:
# #         print(f"\n‚è≥ –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è ({TIMEOUT}s)! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
# #         print("1. –£–≤–µ–ª–∏—á—å—Ç–µ TIMEOUT –≤ —Å–∫—Ä–∏–ø—Ç–µ")
# #         print("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ 'ollama run llama3.1' –∑–∞–ø—É—â–µ–Ω –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ")
# #     except Exception as e:
# #         print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__}")
# #         print("–°–æ–≤–µ—Ç—ã:")
# #         print("- –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'ollama serve' –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –æ–∫–Ω–µ")
# #         print("- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ 'ollama ps' –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")

# # if __name__ == "__main__":
# #     check_connection()



# import requests, json, time

# BASE = "http://localhost:11434"

# def ok(msg): print(f"‚úÖ {msg}")
# def err(msg): print(f"‚ùå {msg}")
# def info(msg): print(f"üîç {msg}")

# def post(path, payload):
#     resp = requests.post(BASE + path, json=payload, timeout=60)
#     # –î–ª—è /api/generate –∏ /api/chat Ollama —Å—Ç—Ä–∏–º–∏—Ç —Å—Ç—Ä–æ–∫–∏ JSON –ø–æ —Å—Ç—Ä–æ–∫–∞–º
#     # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ—Å—Ç—Ä–æ—á–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å –ø–æ—Ç–æ–∫:
#     text = []
#     for line in resp.iter_lines(decode_unicode=True):
#         if not line: continue
#         try:
#             obj = json.loads(line)
#             text.append(obj.get("message", {}).get("content") or obj.get("response") or "")
#         except json.JSONDecodeError:
#             text.append(line)
#     return resp.status_code, "".join(text).strip()

# def main():
#     info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama ({BASE})")
#     r = requests.get(BASE, timeout=10)
#     if r.status_code == 200:
#         ok(f"–°–µ—Ä–≤–µ—Ä: HTTP 200 ({r.text.strip()})")
#     else:
#         return err(f"–°–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: HTTP {r.status_code}")

#     # list models
#     r = requests.get(BASE + "/api/tags", timeout=10)
#     if r.status_code != 200:
#         return err(f"/api/tags: HTTP {r.status_code}")
#     data = r.json()
#     models = [m["name"] for m in data.get("models", [])]
#     print("\nüìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
#     for m in models:
#         print("-", m)
#     if not models:
#         return err("–ù–µ—Ç –º–æ–¥–µ–ª–µ–π. –í—ã–ø–æ–ª–Ω–∏: ollama pull qwen2.5:0.5b-instruct")

#     # test generate
#     print("\nüî• –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ (POST /api/chat)...")
#     code, out = post("/api/chat", {
#         "model": "qwen2.5:0.5b-instruct",
#         "messages": [{"role":"user","content":"–°–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫–æ '–ü—Ä–∏–≤–µ—Ç'"}]
#     })
#     if code == 200 and out:
#         ok(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –û–ö: {out[:120]}")
#     else:
#         err(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: HTTP {code} / –û—Ç–≤–µ—Ç: {out[:200]}")

#     # test embeddings
#     print("\nüß† –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (POST /api/embeddings)...")
#     r = requests.post(BASE + "/api/embeddings", json={
#         "model": "nomic-embed-text",
#         "prompt": "hello world"
#     }, timeout=30)
#     if r.status_code == 200 and "embedding" in r.json():
#         ok("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –û–ö")
#     else:
#         err(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏: HTTP {r.status_code} / {r.text[:200]}")

# if __name__ == "__main__":
#     main()



# ollama_check_and_autostart.py
# check_ollama_connection.py
# -*- coding: utf-8 -*-
import os
import sys
import time
import json
import platform
import subprocess
import shutil
import requests
from typing import List, Set

# ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =====
BASE = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
GEN_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:1.5b-instruct")
EMB_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text:latest")
AUTO_OPEN_INTERACTIVE_RUN = True  # –û—Ç–∫—Ä—ã—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ–µ –æ–∫–Ω–æ —Å `ollama run qwen2.5:1.5b-instruct`

TIMEOUT_SHORT = 10
TIMEOUT_LONG = 60

def ok(msg): print(f"‚úÖ {msg}")
def err(msg): print(f"‚ùå {msg}")
def info(msg): print(f"üîç {msg}")

def _cmd_exists(cmd: str) -> bool:
    return shutil.which(cmd) is not None

# ---------- –°–µ—Ä–≤–µ—Ä ----------
def start_ollama_server_if_needed() -> bool:
    """–ü–æ–¥–Ω—è—Ç—å ollama serve, –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç."""
    try:
        r = requests.get(BASE, timeout=TIMEOUT_SHORT)
        if r.status_code == 200:
            ok(f"Ollama —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ({r.text.strip()})")
            return True
    except requests.RequestException:
        pass

    if not _cmd_exists("ollama"):
        err("–ö–æ–º–∞–Ω–¥–∞ 'ollama' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ PATH. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ.")
        return False

    info("–°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç. –ó–∞–ø—É—Å–∫–∞—é `ollama serve`...")
    try:
        creationflags = 0
        startupinfo = None
        if platform.system() == "Windows":
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            creationflags = subprocess.DETACHED_PROCESS | subprocess.CREATE_NEW_PROCESS_GROUP

        subprocess.Popen(
            ["ollama", "serve"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            startupinfo=startupinfo,
            creationflags=creationflags
        )

        # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        for _ in range(30):
            try:
                r = requests.get(BASE, timeout=2)
                if r.status_code == 200:
                    ok("–°–µ—Ä–≤–µ—Ä Ollama –∑–∞–ø—É—â–µ–Ω.")
                    return True
            except requests.RequestException:
                pass
            time.sleep(1)
        err("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–ø—É—Å–∫–∞ Ollama (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ `ollama serve`).")
        return False
    except Exception as e:
        err(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Ollama: {e}")
        return False

# ---------- –ú–æ–¥–µ–ª–∏ ----------
def _fetch_models() -> List[dict]:
    r = requests.get(f"{BASE}/api/tags", timeout=TIMEOUT_SHORT)
    r.raise_for_status()
    return r.json().get("models", [])

def _norm_variants(name: str) -> Set[str]:
    """–í–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω–∏: base, base:latest, –∏—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞."""
    base = name.split(":", 1)[0]
    return {base, f"{base}:latest", name}

def get_installed_models_set() -> Set[str]:
    """–ú–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏–º—ë–Ω –≤—Å–µ—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    try:
        models = _fetch_models()
    except Exception as e:
        err(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
        return set()

    s: Set[str] = set()
    for m in models:
        raw = m.get("model") or m.get("name")
        if raw:
            if ":" not in raw and "tag" in m:
                raw = f"{raw}:{m['tag']}"
            s |= _norm_variants(raw)
    return s

def ensure_model(model: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ (—Å —É—á—ë—Ç–æ–º :latest), –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ ‚Äî `ollama pull`."""
    installed = get_installed_models_set()
    if model in installed or any(v in installed for v in _norm_variants(model)):
        ok(f"–ú–æ–¥–µ–ª—å —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {model}")
        return True

    info(f"–ú–æ–¥–µ–ª—å '{model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ó–∞–≥—Ä—É–∂–∞—é: `ollama pull {model}`")
    try:
        with subprocess.Popen(["ollama", "pull", model],
                              stdout=subprocess.PIPE,
                              stderr=subprocess.STDOUT,
                              text=True) as p:
            for line in p.stdout:
                print(line.rstrip())
    except FileNotFoundError:
        err("–ö–æ–º–∞–Ω–¥–∞ 'ollama' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ–±–∞–≤—å—Ç–µ Ollama –≤ PATH.")
        return False
    except Exception as e:
        err(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ '{model}': {e}")
        return False

    # –ò–Ω–æ–≥–¥–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–µ —Å—Ä–∞–∑—É ‚Äî –¥–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    for _ in range(8):
        time.sleep(1.0)
        installed = get_installed_models_set()
        if model in installed or any(v in installed for v in _norm_variants(model)):
            ok(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model}")
            return True

    err(f"–ú–æ–¥–µ–ª—å '{model}' –Ω–µ –ø–æ—è–≤–∏–ª–∞—Å—å –ø–æ—Å–ª–µ pull.")
    return False

# ---------- –ü—Ä–æ–≥—Ä–µ–≤ ----------
def warmup_chat(model: str) -> bool:
    """–ù–µ—Å—Ç—Ä–∏–º–∏–æ–≤—ã–π –≤—ã–∑–æ–≤ /api/chat ‚Äî –Ω–∞–¥—ë–∂–Ω–æ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    try:
        r = requests.post(
            f"{BASE}/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": "–°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ: –ü—Ä–∏–≤–µ—Ç"}],
                "stream": False
            },
            timeout=TIMEOUT_LONG
        )
        if r.status_code == 200:
            ok(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç ({model}).")
            return True
        err(f"–û—à–∏–±–∫–∞ chat API: HTTP {r.status_code} ‚Äî {r.text[:200]}")
        return False
    except Exception as e:
        err(f"–°–±–æ–π chat API: {e}")
        return False

def warmup_embeddings(model: str) -> bool:
    try:
        r = requests.post(
            f"{BASE}/api/embeddings",
            json={"model": model, "prompt": "hello world"},
            timeout=TIMEOUT_SHORT
        )
        if r.status_code == 200 and "embedding" in r.json():
            ok(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä–∞–±–æ—Ç–∞—é—Ç ({model}).")
            return True
        err(f"–û—à–∏–±–∫–∞ embeddings API: HTTP {r.status_code} ‚Äî {r.text[:200]}")
        return False
    except Exception as e:
        err(f"–°–±–æ–π embeddings API: {e}")
        return False

# ---------- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π run ----------
def open_interactive_run(model: str):
    """–û—Ç–∫—Ä—ã—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ–µ –æ–∫–Ω–æ —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ—Å—Å–∏–µ–π `ollama run <model>`."""
    if not _cmd_exists("ollama"):
        err("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π run: 'ollama' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    info(f"–û—Ç–∫—Ä—ã–≤–∞—é –æ–∫–Ω–æ —Å `ollama run {model}`...")
    try:
        system = platform.system()
        if system == "Windows":
            subprocess.Popen(["cmd", "/c", "start", "cmd", "/k", f"ollama run {model}"])
        elif system == "Darwin":
            osa = f'tell application "Terminal" to do script "ollama run {model}"'
            subprocess.Popen(["osascript", "-e", osa])
        else:
            term = shutil.which("gnome-terminal") or shutil.which("xterm")
            if term and os.getenv("DISPLAY"):
                if "gnome-terminal" in term:
                    subprocess.Popen([term, "--", "bash", "-lc", f"ollama run {model}"])
                else:
                    subprocess.Popen([term, "-e", f"bash -lc 'ollama run {model}'"])
            else:
                # –±–µ–∑ GUI-—Ç–µ—Ä–º–∏–Ω–∞–ª–∞ ‚Äî –ø—Ä–æ—Å—Ç–æ —Å—Ç–∞—Ä—Ç—É–µ–º —Ñ–æ–Ω–æ–º
                subprocess.Popen(["ollama", "run", model])
        ok("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–∫–Ω–æ –∑–∞–ø—É—â–µ–Ω–æ.")
    except Exception as e:
        err(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π run: {e}")

# ---------- –ì–ª–∞–≤–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π ----------
def print_models_table():
    models = sorted(get_installed_models_set())
    print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
    if not models:
        print("‚Äî")
    else:
        for m in models:
            print(f"- {m}")

def main():
    info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama ({BASE})")
    if not start_ollama_server_if_needed():
        sys.exit(1)

    print_models_table()

    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
    ok_models = True
    ok_models &= ensure_model(GEN_MODEL)
    ok_models &= ensure_model(EMB_MODEL)
    if not ok_models:
        err("–ù–µ –≤—Å–µ —Ç—Ä–µ–±—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å.")
        sys.exit(2)

    # –ü—Ä–æ–≥—Ä–µ–≤ API
    warmup_chat(GEN_MODEL)
    warmup_embeddings(EMB_MODEL)

    # –û—Ç–∫—Ä—ã—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π run –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤—ã –ø—Ä–æ—Å–∏–ª–∏)
    if AUTO_OPEN_INTERACTIVE_RUN:
        open_interactive_run(GEN_MODEL)

    ok("–ì–æ—Ç–æ–≤–æ: Ollama –¥–æ—Å—Ç—É–ø–µ–Ω, –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å, –ø—Ä–æ–≥—Ä–µ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω.")

if __name__ == "__main__":
    main()

